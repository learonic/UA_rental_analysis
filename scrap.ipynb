{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Selector\n",
    "from datetime import date\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_price(str):\n",
    "    \"\"\"This function parses property price from str format to numeric. \n",
    "    It also strips price from spaces, currency id and dots\n",
    "\n",
    "    Args:\n",
    "        str (string): property price in string format scraped from ad\n",
    "\n",
    "    Returns:\n",
    "        int: property price in Ukrainian Hryvnias\n",
    "\n",
    "    Examples:\n",
    "        >>> parse_price('12 000 грн.')\n",
    "        12000\n",
    "    \"\"\"\n",
    "    str_nocurrency = str.replace('грн.', '')\n",
    "    str_clean = str_nocurrency.replace(' ', '').replace('.', '')\n",
    "    return int(str_clean)\n",
    "\n",
    "def parse_tags(tags):\n",
    "    \"\"\"Takes list of tags and retrieves \n",
    "    property details: apartment area, kitchen area, rooms quantity, apartment floor, building height \n",
    "    and posting details: was the ad posted by private person or business\n",
    "\n",
    "    Args:\n",
    "        tags (list): list of tags scraped from ad\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionaty of parsed tags\n",
    "    \"\"\"\n",
    "    tags_dict={'other_tags':[]}\n",
    "    for tag in tags:\n",
    "        if tag == 'Бизнес':\n",
    "            tags_dict['posted_by']='Business'\n",
    "        elif tag == 'Частное лицо':\n",
    "            tags_dict['posted_by']='Private'\n",
    "        elif tag[:9] == \"Этажность\":\n",
    "            try:\n",
    "                tags_dict['building_height_floors']=int(tag.replace('Этажность: ', ''))\n",
    "            except:\n",
    "                tags_dict['other_tags'].append(tag)\n",
    "        elif tag[:4] == \"Этаж\":\n",
    "            try:\n",
    "                tags_dict['floor']=int(tag.replace('Этаж: ', ''))\n",
    "            except:\n",
    "                tags_dict['other_tags'].append(tag)\n",
    "        elif tag[:5] == 'Общая':\n",
    "            try:\n",
    "                tags_dict['apt_area_sqm']=float(tag.replace('Общая площадь: ', '').replace(' м²',''))\n",
    "            except:\n",
    "                tags_dict['other_tags'].append(tag)\n",
    "        elif tag[:13] == 'Площадь кухни':\n",
    "            try:\n",
    "                tags_dict['kitchen_area_sqm'] = float(tag.replace('Площадь кухни: ', '').replace(' м²',''))\n",
    "            except:\n",
    "                tags_dict['other_tags'].append(tag)\n",
    "        elif tag[:17] == 'Количество комнат':\n",
    "            tags_dict['rooms'] = int(tag.replace('Количество комнат: ', '')[0])\n",
    "        else:\n",
    "            tags_dict['other_tags'].append(tag)\n",
    "    return tags_dict  \n",
    "\n",
    "def get_post(url):\n",
    "    \"\"\"This function takes link to ad and returns ad text and tags\n",
    "\n",
    "    Args:\n",
    "        url (str): link to ad\n",
    "\n",
    "    Returns:\n",
    "        tuple: list of tags, ad text\n",
    "    \"\"\"\n",
    "    #retrieves data: text, and tags\n",
    "    property = requests.get(url).text\n",
    "    sel = Selector(text=property)\n",
    "    tags_xpath = '//*[@id=\"root\"]/div[1]/div[3]/div[2]/div[1]/div/ul/li/p/text()'\n",
    "    text_xpath = '//*[@id=\"root\"]/div[1]/div[3]/div[2]/div[1]/div/div[8]/div/text()'\n",
    "    tags = sel.xpath(tags_xpath).extract()\n",
    "    text = sel.xpath(text_xpath).extract()\n",
    "    return (tags, process_post(text))\n",
    "   \n",
    "def process_post(post):\n",
    "    \"\"\"Takes post text as string or list. Combines it i single string and removes special characters\n",
    "\n",
    "    Args:\n",
    "        post (list or str): Olx ad post text\n",
    "\n",
    "    Returns:\n",
    "        str: cleaned olx ad post text\n",
    "    \"\"\"\n",
    "    #Cleans post text. Joins into single string, removes /n\n",
    "    post_str = \"\".join(post)\n",
    "    return post_str.replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "def strip_links(links):\n",
    "    \"\"\"Removes trailing sharp and special information from the link\n",
    "\n",
    "    Args:\n",
    "        links (string): Standard olx link. Looks like this:\n",
    "        https://www.olx.ua/d/uk/obyavlenie/sdam-2k-kvartiru-na-ul-knyazhiy-zaton-v-5-min-metro-osokorki-IDNC3Oo.html#a87b8ce1bb;promoted\n",
    "\n",
    "    Returns:\n",
    "        str: clean olx link\n",
    "\n",
    "    Examples:\n",
    "        >>> strip_links('https://www.olx.ua/d/uk/obyavlenie/sdam-2k-kvartiru-na-ul-knyazhiy-zaton-v-5-min-metro-osokorki-IDNC3Oo.html#a87b8ce1bb;promoted')\n",
    "        'https://www.olx.ua/d/uk/obyavlenie/sdam-2k-kvartiru-na-ul-knyazhiy-zaton-v-5-min-metro-osokorki-IDNC3Oo.html'\n",
    "    \"\"\"\n",
    "    return [st.split('#',1)[0] for st in links]\n",
    "\n",
    "def get_links(page):\n",
    "    \"\"\"This functions takes content of olx summary page or search results and returns tuples of links and prices for each object on the page\n",
    "\n",
    "    Args:\n",
    "        page (str): Content of olx page\n",
    "\n",
    "    Returns:\n",
    "        zip: zip object generaiting tuples of (link, price) for each property shown on page\n",
    "    \"\"\"\n",
    "    sel = Selector(text=page)\n",
    "    item_xpath = '//*[@id=\"offers_table\"]/tbody/tr/td/div[@class=\"offer-wrapper\"]'\n",
    "    item_link_xpath='.//h3/a/@href'\n",
    "    item_price_xpath='./table/tbody/tr[1]/td[3]/div/p/strong/text()'\n",
    "    links=sel.xpath(item_xpath).xpath(item_link_xpath).extract()\n",
    "    prices=sel.xpath(item_xpath).xpath(item_price_xpath).extract()\n",
    "    prices_clean = map(parse_price, prices)\n",
    "    links_clean=strip_links(links)\n",
    "    return zip(links_clean, prices_clean)\n",
    "\n",
    "def get_pages_qty(page):\n",
    "    \"\"\"This function takes search results page and returns quantity of pages in search\n",
    "\n",
    "    Args:\n",
    "        page (str): content of the search page\n",
    "\n",
    "    Returns:\n",
    "        int: quantity of pages in search\n",
    "    \"\"\"\n",
    "    sel = Selector(text=page)\n",
    "    link_xpath='//*[@id=\"body-container\"]/div[3]/div/div[6]/span[16]/a/span/text()'\n",
    "    return int(sel.xpath(link_xpath).extract_first())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.olx.ua/nedvizhimost/kvartiry/dolgosrochnaya-arenda-kvartir/' #Starting link\n",
    "today = date.today()\n",
    "try:\n",
    "    data = pd.read_csv('Data/rent_offers.csv') #reading file\n",
    "except:\n",
    "    data=pd.DataFrame([],columns=['url', 'price', 'city', 'tags', 'description']) # empty df, used for the first run\n",
    "new_data=[]\n",
    "cities=['Kiev', 'Lvov', 'Odessa', 'Kharkov', 'Dnepr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m city_url \u001b[39m=\u001b[39m base_url \u001b[39m+\u001b[39m city \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/?page=\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      3\u001b[0m r\u001b[39m=\u001b[39mrequests\u001b[39m.\u001b[39mget(city_url\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m last_page \u001b[39m=\u001b[39m get_pages_qty(r\u001b[39m.\u001b[39;49mtext)\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m tqdm_notebook(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, last_page\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), desc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mScrapping \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m city):\n\u001b[0;32m      6\u001b[0m     url \u001b[39m=\u001b[39m city_url \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(page)\n",
      "Cell \u001b[1;32mIn[5], line 139\u001b[0m, in \u001b[0;36mget_pages_qty\u001b[1;34m(page)\u001b[0m\n\u001b[0;32m    137\u001b[0m sel \u001b[39m=\u001b[39m Selector(text\u001b[39m=\u001b[39mpage)\n\u001b[0;32m    138\u001b[0m link_xpath\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m//*[@id=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbody-container\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]/div[3]/div/div[6]/span[16]/a/span/text()\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39;49m(sel\u001b[39m.\u001b[39;49mxpath(link_xpath)\u001b[39m.\u001b[39;49mextract_first())\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "for city in cities:\n",
    "    city_url = base_url + city + '/?page='\n",
    "    r=requests.get(city_url+'1')\n",
    "    last_page = get_pages_qty(r.text)\n",
    "    for page in tqdm_notebook(range(1, last_page+1), desc = 'Scrapping ' + city):\n",
    "        url = city_url + str(page)\n",
    "        pages = get_links(requests.get(url).content)\n",
    "        for url, price in pages:\n",
    "            if url in data.url.values: #checking if record already esists\n",
    "                if data[data.url == url].price.values[0] != price:\n",
    "                    data[data.url == url].price = price\n",
    "            else:\n",
    "                new_data.append([url, price, city, today, *get_post(url)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "upd_data = pd.DataFrame(new_data, columns=['url', 'price','city', 'date', 'tags', 'description']) #converting to Dataframe \n",
    "tags_df = pd.DataFrame(upd_data.tags.map(parse_tags).tolist()) #parsing tags\n",
    "parsed_data = pd.concat([upd_data, tags_df], axis=1).drop(['tags'], axis=1) #join together & drop initial tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=pd.concat([data,parsed_data],ignore_index=True)\n",
    "result.to_csv('Data/rent_offers.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d9db9b98d71c629d8bc8ba48ca21d6d314bfda293109023b2fb1d805869e782"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('scrap': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
